{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aaryan Shetty\\.virtualenvs\\Demo_folder-AKx2sSPh\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# import Libraries\n",
    "\n",
    "import openai\n",
    "import langchain\n",
    "import pinecone \n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import google.generativeai \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading env variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Read the document\n",
    "def read_doc(directory):\n",
    "    file_loader=PyPDFDirectoryLoader(directory)\n",
    "    documents=file_loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=read_doc('../pdf/')\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide the docs into chunks\n",
    "### https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html#\n",
    "def chunk_data(docs,chunk_size=800,chunk_overlap=50):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    doc=text_splitter.split_documents(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=chunk_data(docs=doc)\n",
    "documents\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000028A7D9C7D90> model='models/embedding-001' task_type=None google_api_key=None credentials=None client_options=None transport=None request_options=None\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize embeddings with API key from .env file\n",
    "api_key = os.getenv('GOOGLE_API_KEY')\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", api_key=api_key)\n",
    "\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello AIzaSyCOS49bP_Qtop-cQ9xM4AkqvnUr2bqSgBg\n",
      "The dimension of the embedding vector is: 768\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize embeddings with API key from .env file\n",
    "api_key = os.getenv('GOOGLE_API_KEY')\n",
    "print(\"hello\",api_key)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", api_key=api_key)\n",
    "\n",
    "# Generate an embedding for a sample text to determine the dimensions\n",
    "sample_text = \"This is a sample text to determine embedding dimensions.\"\n",
    "embedding_vector = embeddings.embed_query(sample_text)\n",
    "\n",
    "# Get the dimension of the embedding vector\n",
    "embedding_dimension = len(embedding_vector)\n",
    "print(f\"The dimension of the embedding vector is: {embedding_dimension}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_client = pinecone.Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"), environment=os.getenv(\"PINECONE_ENVIRONMENT\"))\n",
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index = pc.Index(\"check1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 237/237 [03:52<00:00,  1.02it/s]\n",
      "100%|██████████| 3/3 [00:47<00:00, 15.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 237 vectors to Pinecone index \n",
      "Uploaded 237 vectors to Pinecone index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import uuid\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Function to convert documents to vectors\n",
    "def docs_to_vectors(docs: List[Document], embeddings: GoogleGenerativeAIEmbeddings):\n",
    "    vectors = []\n",
    "    for doc in tqdm(docs):\n",
    "        embedding = embeddings.embed_query(doc.page_content)\n",
    "        metadata = doc.metadata\n",
    "        metadata['text'] = doc.page_content\n",
    "        vectors.append((doc.metadata.get('source', 'unknown'), embedding, metadata))\n",
    "    return vectors\n",
    "\n",
    "# Function to upload vectors to Pinecone\n",
    "def upload_to_pinecone(vectors, index):\n",
    "    batch_size = 100\n",
    "    for i in tqdm(range(0, len(vectors), batch_size)):\n",
    "        batch = vectors[i:i+batch_size]\n",
    "        \n",
    "        # Generate unique IDs for each vector in the batch\n",
    "        ids = [str(uuid.uuid4()) for _ in range(len(batch))]\n",
    "        \n",
    "        # Prepare the vectors with unique IDs\n",
    "        vector_batch = [(ids[j], vec, meta) for j, (_, vec, meta) in enumerate(batch)]\n",
    "        \n",
    "        # Upsert the batch\n",
    "        index.upsert(vectors=vector_batch)\n",
    "\n",
    "    print(f\"Uploaded {len(vectors)} vectors to Pinecone index \")\n",
    "\n",
    "# Generate embeddings and convert to vectors\n",
    "vectors = docs_to_vectors(documents, embeddings)\n",
    "\n",
    "# Upload vectors to Pinecone\n",
    "upload_to_pinecone(vectors, index)\n",
    "\n",
    "print(f\"Uploaded {len(vectors)} vectors to Pinecone index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Create a vector store from the Pinecone index\n",
    "vectorstore = LangchainPinecone.from_existing_index(index_name=\"check1\", embedding=embeddings)\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\", temperature=0.5)\n",
    "\n",
    "# Create a custom prompt template\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer: \"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "# Function to query the RAG chain\n",
    "def query_rag(question):\n",
    "    result = qa_chain({\"query\": question})\n",
    "    return result[\"result\"], result[\"source_documents\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aaryan Shetty\\.virtualenvs\\Demo_folder-AKx2sSPh\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can you talk about Apportionment of benefit of obligation on severance\n",
      "Answer: When a property is divided and held in several shares due to a transfer, the benefit of any obligation related to the entire property, which passes from one to several owners, comes under the Apportionment of benefit of obligation on severance. This section of the Transfer of Property Act, 1882 explains that in such cases, the corresponding duty shall be performed in favor of each owner in proportion to the value of their share in the property. \n",
      "\n",
      "However, this applies only if:\n",
      "\n",
      "* There is no contract to the contrary among the owners.\n",
      "* The duty can be severed.\n",
      "* Severance does not substantially increase the burden of the obligation.\n",
      "\n",
      "If the duty cannot be severed or if severance would significantly increase the burden of the obligation, the duty shall be performed for the benefit of one owner designated by all the owners.\n",
      "\n",
      "Furthermore, the person bearing the burden of the obligation is not liable for failing to discharge it as provided by this section unless they have received reasonable notice of the severance.\n",
      "\n",
      "This section does not apply to leases for agricultural purposes unless the State Government notifies otherwise in the Official Gazette.\n",
      "\n",
      "\n",
      "Sources:\n",
      "Source 1: ..\\pdf\\trp_act.pdf\n",
      "Source 2: ..\\pdf\\trp_act.pdf\n",
      "Source 3: ..\\pdf\\The_Indian_Contract_Act_1872.PDF\n",
      "Source 4: ..\\pdf\\The_Indian_Contract_Act_1872.PDF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"Can you talk about Apportionment of benefit of obligation on severance\"\n",
    "answer, sources = query_rag(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(\"\\nSources:\")\n",
    "for i, doc in enumerate(sources):\n",
    "    print(f\"Source {i+1}: {doc.metadata.get('source', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##To delete all records in the vector store\n",
    "#index.delete(delete_all=True, namespace='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Demo_folder-AKx2sSPh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
